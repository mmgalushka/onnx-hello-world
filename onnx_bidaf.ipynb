{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use ONNX Bidirectional Attention Flow\n",
    "\n",
    "\n",
    " In this notebook, we investigate how to use the [BiDAF](https://arxiv.org/abs/1611.01603) natural language processing model serialized in [ONNX](https://onnx.ai/) format. For performing this experiment, we will use some random contexts and queries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Environment\n",
    "\n",
    "But before we start, let's set up our working environment. It will help us to keep the project directory clean. All artifacts generated within the project will be placed to the \"tmp\" dir and will be ignored by Git."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Let's make a temporary directory for all artifacts created by this notebook.\n",
    "import os\n",
    "if not os.path.exists('tmp'):\n",
    "    os.makedirs('tmp')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mykola/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download ResNet ONNX Model\n",
    "\n",
    "Use the following [page](https://github.com/onnx/models/tree/master/text/machine_comprehension/bidirectional_attention_flow) to get the URL for the BiDAF ONNX model.\n",
    "\n",
    "**Please note that downloading ONNX models have a large size. To prevent the repeated download of the same model on the notebook restart we check whether the model already exists. If a model exists we skip the download. To force download, go to the \"tmp\" directory and delete the \"bidaf.onnx\" file.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_name = 'bidaf'\n",
    "if not os.path.exists(os.path.join('tmp', f'{model_name}.onnx')):\n",
    "    !wget -O tmp/bidaf.onnx https://github.com/onnx/models/raw/master/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx \n",
    "else:\n",
    "    print('This notebook is using an already downloaded model.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This notebook is using an already downloaded model.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Context/Query Preprocessor\n",
    "\n",
    "Before using BiDAF, we need to preprocess the context/query using the predefined function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def preprocess(text):\n",
    "   tokens = word_tokenize(text)\n",
    "   # split into lower-case word tokens, in numpy array with shape of (seq, 1)\n",
    "   words = np.asarray([w.lower() for w in tokens]).reshape(-1, 1)\n",
    "   # split words into chars, in numpy array with shape of (seq, 1, 1, 16)\n",
    "   chars = [[c for c in t][:16] for t in tokens]\n",
    "   chars = [cs+['']*(16-len(cs)) for cs in chars]\n",
    "   chars = np.asarray(chars).reshape(-1, 1, 1, 16)\n",
    "   return words, chars"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Answer\n",
    "\n",
    "First we need to load the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import onnxruntime as rt\n",
    "sess = rt.InferenceSession(os.path.join('tmp', 'bidaf.onnx'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-16 09:29:46.300242242 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300294131 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300304003 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300307810 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300311377 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300314931 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-16 09:29:46.300318248 [W:onnxruntime:, graph.cc:1074 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create input (context and query)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "context = '''\n",
    "Specifically, the hottest spot ever recorded on Earth is El Azizia, in Libya,\n",
    "where a temperature of 136 degrees Fahrenheit was recorded on Sept. 13, 1922.\n",
    "While hotter spots have likely occurred in other parts of the planet at other\n",
    "times, this is the most scorching temperature ever formally recorded by a\n",
    "weather station. \n",
    "'''\n",
    "query = 'Where the hottest spot on Earth?'\n",
    "cw, cc = preprocess(context)\n",
    "qw, qc = preprocess(query)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate answer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "response = sess.run([], {'context_word':cw, 'context_char':cc, 'query_word':qw, 'query_char':qc})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the answer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "answer = []\n",
    "starts, ends = response\n",
    "for start, end in zip(starts, ends):\n",
    "    s = np.asscalar(start)\n",
    "    e = np.asscalar(end)\n",
    "    answer.append(' '.join([w for w in cw[s:e+1].reshape(-1)]))\n",
    "\n",
    "print('Query:  %s' % query)\n",
    "print('Answer: %s' % ', '.join(answer))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query:  Where the hottest spot on Earth?\n",
      "Answer: el azizia\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5b01ea270aa5259acbf4f8381f599d977e3036a7ea54781fba0ae6d431406d64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}