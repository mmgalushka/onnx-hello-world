{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use ONNX Bidirectional Attention Flow\n",
    "\n",
    "\n",
    " In this notebook, we investigate how to use the [BiDAF](https://arxiv.org/abs/1611.01603) natural language processing model serialized in [ONNX](https://onnx.ai/) format. For performing this experiment, we will use some random contexts and queries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Environment\n",
    "\n",
    "But before we start, let's set up our working environment. It will help us to keep the project directory clean. All artifacts generated within the project will be placed to the \"tmp\" dir and will be ignored by Git."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Let's make a temporary directory for all artifacts created by this notebook.\n",
    "import os\n",
    "if not os.path.exists('tmp'):\n",
    "    os.makedirs('tmp')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download ResNet ONNX Model\n",
    "\n",
    "Use the following [page](https://github.com/onnx/models/tree/master/text/machine_comprehension/bidirectional_attention_flow) to get the URL for the BiDAF ONNX model.\n",
    "\n",
    "**Please note that downloading ONNX models have a large size. To prevent the repeated download of the same model on the notebook restart we check whether the model already exists. If a model exists we skip the download. To force download, go to the \"tmp\" directory and delete the \"bidaf.onnx\" file.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if not os.path.exists(os.path.join('tmp', 'bidaf.onnx')):\n",
    "    !wget -O tmp/resnet.onnx https://github.com/onnx/models/raw/master/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx \n",
    "                      "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-09-14 13:10:18--  https://github.com/onnx/models/raw/master/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/onnx/models/master/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx [following]\n",
      "--2021-09-14 13:10:18--  https://media.githubusercontent.com/media/onnx/models/master/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43522228 (42M) [application/octet-stream]\n",
      "Saving to: ‘tmp/resnet.onnx’\n",
      "\n",
      "tmp/resnet.onnx     100%[===================>]  41.51M  3.11MB/s    in 16s     \n",
      "\n",
      "2021-09-14 13:10:37 (2.53 MB/s) - ‘tmp/resnet.onnx’ saved [43522228/43522228]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Context/Query Preprocessor\n",
    "\n",
    "Before using BiDAF, we need to preprocess the context/query using the predefined function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess(text):\n",
    "   tokens = word_tokenize(text)\n",
    "   # split into lower-case word tokens, in numpy array with shape of (seq, 1)\n",
    "   words = np.asarray([w.lower() for w in tokens]).reshape(-1, 1)\n",
    "   # split words into chars, in numpy array with shape of (seq, 1, 1, 16)\n",
    "   chars = [[c for c in t][:16] for t in tokens]\n",
    "   chars = [cs+['']*(16-len(cs)) for cs in chars]\n",
    "   chars = np.asarray(chars).reshape(-1, 1, 1, 16)\n",
    "   return words, chars"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mykola/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Answer\n",
    "\n",
    "First we need to load the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(os.path.join('tmp', 'bidaf.onnx'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-14 12:15:56.811330155 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811350563 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811354731 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811358156 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811375927 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811379169 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-09-14 12:15:56.811382371 [W:onnxruntime:, graph.cc:1074 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create input (context and query)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "context = '''\n",
    "Specifically, the hottest spot ever recorded on Earth is El Azizia, in Libya,\n",
    "where a temperature of 136 degrees Fahrenheit was recorded on Sept. 13, 1922.\n",
    "While hotter spots have likely occurred in other parts of the planet at other\n",
    "times, this is the most scorching temperature ever formally recorded by a\n",
    "weather station. \n",
    "'''\n",
    "query = 'By which station it was recorded?'\n",
    "cw, cc = preprocess(context)\n",
    "qw, qc = preprocess(query)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate answer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "answer = sess.run([], {'context_word':cw, 'context_char':cc, 'query_word':qw, 'query_char':qc})\n",
    "answer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([58], dtype=int32), array([59], dtype=int32)]"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# assuming answer contains the np arrays for start_pos/end_pos\n",
    "starts, ends = answer\n",
    "for start, end in zip(starts, ends):\n",
    "    s = np.asscalar(start)\n",
    "    e = np.asscalar(end)\n",
    "    print([w for w in cw[s:e+1].reshape(-1)])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['weather', 'station']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_11145/1183086202.py:4: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  s = np.asscalar(start)\n",
      "/tmp/ipykernel_11145/1183086202.py:5: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  e = np.asscalar(end)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5b01ea270aa5259acbf4f8381f599d977e3036a7ea54781fba0ae6d431406d64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}